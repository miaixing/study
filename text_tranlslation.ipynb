{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\software\\conda\\envs\\mnist\\lib\\site-packages\\torchtext\\data\\__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "d:\\software\\conda\\envs\\mnist\\lib\\site-packages\\torchtext\\vocab\\__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "d:\\software\\conda\\envs\\mnist\\lib\\site-packages\\torchtext\\utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "d:\\software\\conda\\envs\\mnist\\lib\\site-packages\\torchtext\\datasets\\__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import multi30k, Multi30k\n",
    "from typing import Iterable, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
    "multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
    "\n",
    "src_language = 'de'\n",
    "tgt_language = 'en'\n",
    "\n",
    "token_transform = {}\n",
    "vocab_transform = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_transform[src_language] = get_tokenizer(tokenizer='spacy', language='de_core_news_sm')\n",
    "token_transform[tgt_language] = get_tokenizer(tokenizer='spacy', language='en_core_web_sm' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\software\\conda\\envs\\mnist\\lib\\site-packages\\torchdata\\datapipes\\__init__.py:18: UserWarning: \n",
      "################################################################################\n",
      "WARNING!\n",
      "The 'datapipes', 'dataloader2' modules are deprecated and will be removed in a\n",
      "future torchdata release! Please see https://github.com/pytorch/data/issues/1196\n",
      "to learn more and leave feedback.\n",
      "################################################################################\n",
      "\n",
      "  deprecation_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished. en_vocab_len:10837. de_vocab_len:19214.\n",
      "[1166, 3426, 3930, 742]\n"
     ]
    }
   ],
   "source": [
    "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
    "    language_index = {\n",
    "        src_language: 0,\n",
    "        tgt_language: 1\n",
    "    }\n",
    "    for from_to_tuple in data_iter:\n",
    "        yield token_transform[language](from_to_tuple[language_index[language]])\n",
    "\n",
    "unk_index, pad_index, bos_index, eos_index = 0, 1, 2, 3\n",
    "special_tokens = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "for lan in [src_language, tgt_language]:\n",
    "    train_data_pipe = Multi30k(split='train', language_pair=(src_language, tgt_language))\n",
    "    vocab_transform[lan] = build_vocab_from_iterator(\n",
    "        iterator=yield_tokens(data_iter=train_data_pipe, language=lan),\n",
    "        specials= special_tokens,\n",
    "        special_first=      True\n",
    "    )\n",
    "\n",
    "for lan in [src_language, tgt_language]:\n",
    "    vocab_transform[lan].set_default_index(unk_index)\n",
    "\n",
    "print('Finished. en_vocab_len:{}. de_vocab_len:{}.'.format(len(vocab_transform[tgt_language]), len(vocab_transform[src_language])))\n",
    "print(vocab_transform[tgt_language](['I', 'am', 'your', 'father']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "from torch import Tensor\n",
    "from torch import nn\n",
    "from torch.nn import Transformer\n",
    "import math\n",
    "\n",
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmBedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmBedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=emb_size)\n",
    "        self.emd_size = emb_size\n",
    "\n",
    "    def forward(self, token: Tensor):\n",
    "        return self.embedding(token.long())*math.sqrt(self.emd_size)\n",
    "    \n",
    "# test\n",
    "# tokens = [1, 2, 3, 4, 5]\n",
    "# tokens = torch.Tensor(tokens)\n",
    "# print(TokenEmBedding(10, 3)(tokens))\n",
    "\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, emd_size: int, dropout: float, maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(-torch.arange(0, emd_size, 2)*math.log(10000)/emd_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(shape=(maxlen, 1))\n",
    "        pos_embedding = torch.zeros(size=(maxlen, emd_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos*den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos*den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "    \n",
    "# test positional encoding.\n",
    "# tokens = [1, 2, 3, 4, 5]\n",
    "# tokens = torch.Tensor(tokens)\n",
    "# token_embed = TokenEmBedding(10, 4)(tokens)\n",
    "# print(token_embed.shape)\n",
    "# positional_encoder = PositionalEncoding(emd_size=4, dropout=0.3)\n",
    "# print(positional_encoder(token_embed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2seqTransformer(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 num_of_encoder_layer: int,\n",
    "                 num_of_decoder_layer: int,\n",
    "                 emd_size: int,\n",
    "                 n_head: int,\n",
    "                 src_embed_size: int,\n",
    "                 tgt_embed_size: int,\n",
    "                 dim_feed_forward: int,\n",
    "                 dropout: float = 0.1):\n",
    "        super(Seq2seqTransformer, self).__init__()\n",
    "        self.transformer = Transformer(\n",
    "            d_model=emd_size,\n",
    "            nhead=n_head,\n",
    "            num_encoder_layers=num_of_encoder_layer,\n",
    "            num_decoder_layers=num_of_decoder_layer,\n",
    "            dim_feedforward=dim_feed_forward,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.generator = nn.Linear(emd_size, tgt_embed_size)\n",
    "        self.src_token_embed = TokenEmBedding(src_embed_size, emd_size)\n",
    "        self.tgt_token_embed = TokenEmBedding(tgt_embed_size, emd_size)\n",
    "        self.positional_encoding = PositionalEncoding(emd_size=emd_size, dropout=dropout)\n",
    "    \n",
    "    def forward(self, src: Tensor, tgt: Tensor, \n",
    "                src_mask: Tensor, tgt_mask: Tensor, \n",
    "                src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor,\n",
    "                memory_padding_mask: Tensor):\n",
    "        src_embed = self.positional_encoding(self.src_token_embed(src))\n",
    "        tgt_embed = self.positional_encoding(self.tgt_token_embed(tgt))\n",
    "        out = self.transformer(src_embed, tgt_embed, src_mask, tgt_mask, None,\n",
    "                               src_padding_mask, tgt_padding_mask, memory_padding_mask)\n",
    "        return self.generator(out)\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        src_embed = self.positional_encoding(self.src_token_embed(src))\n",
    "        return self.transformer.encoder(src_embed, src_mask)\n",
    "    \n",
    "    def decode(self, tgt, memory, tgt_mask):\n",
    "        tgt_embed = self.positional_encoding(self.tgt_token_embed(tgt))\n",
    "        return self.transformer.decoder(tgt_embed, memory, tgt_mask)\n",
    "    \n",
    "# test seq_seqtransformer\n",
    "# model = Seq2seqTransformer(6, 6, 512, 8, 512, 512, 512)\n",
    "# x = torch.ones(size=(10, 10))\n",
    "# pre = model(src=x,\n",
    "#             tgt=x,\n",
    "#             src_mask=x,\n",
    "#             tgt_mask=x,\n",
    "#             src_padding_mask=x,\n",
    "#             tgt_padding_mask=x,\n",
    "#             memory_padding_mask=x)\n",
    "# pre.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_subsequent_mask(dim: int):\n",
    "    mask = torch.triu(torch.ones(size=(dim, dim), device=device) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask==0, float('-inf')).masked_fill(mask==1, float(0.0))\n",
    "\n",
    "    return mask\n",
    "\n",
    "# # test\n",
    "# mask = generate_subsequent_mask(dim=10)\n",
    "# mask\n",
    "def generate_mask(src, tgt):\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    src_mask = torch.zeros(size=(src_seq_len, src_seq_len), device=device).type(dtype=torch.bool)\n",
    "    tgt_mask = generate_subsequent_mask(dim=tgt_seq_len)\n",
    "\n",
    "    src_padding_mask = (src == pad_index).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == pad_index).transpose(0, 1)\n",
    "\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n",
    "\n",
    "# test\n",
    "# src = torch.Tensor([[1, 3, 4, 5, 0]])\n",
    "# tgt = src\n",
    "# masks = generate_mask(src, tgt)\n",
    "# masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence \n",
    "\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(text_input):\n",
    "        for transform in transforms:\n",
    "            text_input = transform(text_input)\n",
    "        return text_input\n",
    "    return func\n",
    "\n",
    "def tensor_transform(token_ids: List[int]):\n",
    "    return torch.cat((\n",
    "        torch.tensor([eos_index]),\n",
    "        torch.tensor(token_ids  ),\n",
    "        torch.tensor([eos_index])\n",
    "    ))\n",
    "\n",
    "text_tranforms = {}\n",
    "for lan in [src_language, tgt_language]:\n",
    "    text_tranforms[lan] = sequential_transforms(\n",
    "        token_transform[lan],\n",
    "        vocab_transform[lan],\n",
    "        tensor_transform\n",
    "    )\n",
    "\n",
    "def collate_batch(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_samples, tgt_samples in batch:\n",
    "        src_batch.append(text_tranforms[src_language](src_samples.rstrip('\\n')))\n",
    "        tgt_batch.append(text_tranforms[tgt_language](tgt_samples.rstrip('\\n')))\n",
    "\n",
    "    src_batch = pad_sequence(sequences=src_batch, padding_value=pad_index)\n",
    "    tgt_batch = pad_sequence(sequences=tgt_batch, padding_value=pad_index)\n",
    "\n",
    "    return src_batch, tgt_batch\n",
    "\n",
    "# test\n",
    "# train_dp = Multi30k(split='train', language_pair=(src_language, tgt_language))\n",
    "# from torch.utils.data import DataLoader\n",
    "# train_data_loader = DataLoader(dataset=train_dp, batch_size=8, collate_fn=collate_batch)\n",
    "# i = 0\n",
    "# for src, tgt in train_data_loader:\n",
    "#         print(src)\n",
    "#         if i > 5:\n",
    "#              break\n",
    "#         i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the train and valid dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "train_dp = Multi30k(split='train', language_pair=(src_language, tgt_language))\n",
    "valid_dp = Multi30k(split='valid', language_pair=(src_language, tgt_language))\n",
    "\n",
    "train_data_loader = DataLoader(dataset=train_dp, batch_size=BATCH_SIZE, collate_fn=collate_batch)\n",
    "valid_data_loader = DataLoader(dataset=valid_dp, batch_size=BATCH_SIZE, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\software\\conda\\envs\\mnist\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "SRC_VOCAB_SIZE = len(vocab_transform[src_language])\n",
    "TGT_VOCAB_SIZE = len(vocab_transform[tgt_language])\n",
    "EMD_SIZE = 512\n",
    "FFN_HIDEN_DIM = 512\n",
    "N_HEAD = 8\n",
    "NUM_DECODERS = 3\n",
    "NUM_ENCODERS = 3\n",
    "\n",
    "transformer = Seq2seqTransformer(\n",
    "    num_of_decoder_layer=NUM_ENCODERS,\n",
    "    num_of_encoder_layer=NUM_DECODERS,\n",
    "    emd_size=EMD_SIZE,\n",
    "    n_head=N_HEAD,\n",
    "    src_embed_size=SRC_VOCAB_SIZE,\n",
    "    tgt_embed_size=TGT_VOCAB_SIZE,\n",
    "    dim_feed_forward=FFN_HIDEN_DIM\n",
    ")\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        torch.nn.init.xavier_uniform_(p)\n",
    "\n",
    "model = transformer.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=pad_index)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), betas=(0.9, 0.98), lr=0.0001, eps=1e-9)\n",
    "\n",
    "def train_epoch(model, train_data_loader, loss_fn):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    for src, tgt in train_data_loader:\n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "        tgt_input = tgt[:-1, :]\n",
    "        src_mask, tgt_mask, src_pad_mask, tgt_pad_mask = generate_mask(src=src, tgt=tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask, src_pad_mask, tgt_pad_mask, src_pad_mask)\n",
    "        tgt_out = tgt[1:, :]\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses += loss\n",
    "\n",
    "    return losses / len(list(train_data_loader))\n",
    "    \n",
    "def valid_epoch(model, valid_data_loader, loss_fn):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "    for src, tgt in valid_data_loader:\n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "        tgt_input = tgt[:-1, :]\n",
    "        src_mask, tgt_mask, src_pad_mask, tgt_pad_mask = generate_mask(src=src, tgt=tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask, src_pad_mask, tgt_pad_mask, src_pad_mask)\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "\n",
    "        losses += loss\n",
    "\n",
    "    return losses / len(list(valid_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------Begin training-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\software\\conda\\envs\\mnist\\lib\\site-packages\\torch\\utils\\data\\datapipes\\iter\\combining.py:337: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
      "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n",
      "d:\\software\\conda\\envs\\mnist\\lib\\site-packages\\torch\\nn\\functional.py:5137: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training loss:3.762, Valid: 3.315, Time = 381.746s.\n",
      "Epoch: 2, Training loss:3.161, Valid: 2.895, Time = 376.028s.\n",
      "Epoch: 3, Training loss:2.770, Valid: 2.648, Time = 381.729s.\n",
      "Epoch: 4, Training loss:2.480, Valid: 2.444, Time = 372.120s.\n",
      "Epoch: 5, Training loss:2.253, Valid: 2.307, Time = 381.671s.\n",
      "Epoch: 6, Training loss:2.062, Valid: 2.196, Time = 393.447s.\n",
      "Epoch: 7, Training loss:1.900, Valid: 2.119, Time = 382.635s.\n",
      "Epoch: 8, Training loss:1.757, Valid: 2.066, Time = 392.775s.\n",
      "Epoch: 9, Training loss:1.637, Valid: 2.013, Time = 381.642s.\n",
      "Epoch: 10, Training loss:1.524, Valid: 1.960, Time = 393.286s.\n",
      "Epoch: 11, Training loss:1.425, Valid: 1.973, Time = 382.226s.\n",
      "Epoch: 12, Training loss:1.332, Valid: 1.969, Time = 394.054s.\n",
      "Epoch: 13, Training loss:1.252, Valid: 1.943, Time = 382.535s.\n",
      "Epoch: 14, Training loss:1.177, Valid: 1.916, Time = 393.218s.\n",
      "Epoch: 15, Training loss:1.108, Valid: 1.917, Time = 382.967s.\n",
      "Epoch: 16, Training loss:1.041, Valid: 1.930, Time = 393.085s.\n",
      "Epoch: 17, Training loss:0.975, Valid: 1.931, Time = 381.531s.\n",
      "Epoch: 18, Training loss:0.920, Valid: 1.949, Time = 391.855s.\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "NUM_EPOCH = 18\n",
    "def train():\n",
    "    print('------------Begin training-------------')\n",
    "    for epoch in range(1, NUM_EPOCH+1):\n",
    "        start = timer()\n",
    "        train_loss = train_epoch(model=model, train_data_loader=train_data_loader, loss_fn=loss_fn)\n",
    "        valid_loss = valid_epoch(model=model, valid_data_loader=valid_data_loader, loss_fn=loss_fn)\n",
    "        end_time = timer()\n",
    "        print(f\"Epoch: {epoch}, Training loss:{train_loss:.3f}, Valid: {valid_loss:.3f}, Time = {end_time-start:.3f}s.\")\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model: torch.nn.Module, src, src_mask, max_len=100, start_symbol=bos_index):\n",
    "    src = src.to(device)\n",
    "    src_mask = src_mask.to(device)\n",
    "\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(size=(1, 1)).fill_(start_symbol).long().to(device)\n",
    "    for i in range(max_len):\n",
    "        memory = memory.to(device)\n",
    "        tgt_mask = generate_subsequent_mask(dim=ys.shape[0]).type(torch.bool).to(device)\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_token = torch.max(prob, dim=1)\n",
    "\n",
    "        ys = torch.cat(\n",
    "            [ys, torch.ones(1, 1).type_as(src.data).fill_(next_token.squeeze(dim=0))], dim=0\n",
    "        )\n",
    "\n",
    "        if next_token == eos_index:\n",
    "            break\n",
    "\n",
    "    return ys\n",
    "\n",
    "def translate(model: torch.nn.Module, src_sentence: str):\n",
    "    model.eval()\n",
    "    src = text_tranforms[src_language](src_sentence.rstrip('\\n')).view(-1, 1)\n",
    "    mask_dim = src.shape[0]\n",
    "    src_mask = (torch.zeros(size=(mask_dim, mask_dim)).type(torch.bool))\n",
    "    ys = greedy_decode(model=model, src=src, src_mask=src_mask)\n",
    "\n",
    "    sentence = []\n",
    "    for token in ys:\n",
    "        word = vocab_transform[tgt_language].lookup_tokens(token.cpu().numpy())\n",
    "        sentence += word\n",
    "        if word != '<eos>':\n",
    "            sentence += ' '\n",
    "\n",
    "    return \"\".join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos> people stand in front of an auditorium . <eos> '"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(model, \"Eine Gruppe von Menschen steht vor einem Iglu .\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mnist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
